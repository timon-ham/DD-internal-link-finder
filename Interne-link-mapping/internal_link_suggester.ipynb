{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "867abca7",
            "metadata": {},
            "source": [
                "# Internal Link Opportunity Finder v3 (Fix)\n",
                "\n",
                "**Fix applied:**\n",
                "*   **Menu/Footer Ignore**: Now removes navigation menus and footers *before* checking if a link already exists. This prevents the tool from skipping pages just because the target link is in the site-wide menu.\n",
                "\n",
                "**How to use:**\n",
                "1.  Fill in the **Quick Setup** below.\n",
                "2.  Run the **Engine** cell.\n",
                "3.  Download the **multi-sheet Excel file** at the end."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e02e9788",
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title üöÄ Quick Setup\n",
                "#@markdown Enter the website details.\n",
                "ROOT_URL = \"https://example.com\" #@param {type:\"string\"}\n",
                "TARGET_URL_TO_BOOST = \"https://example.com/target-page\" #@param {type:\"string\"}\n",
                "ANCHOR_TEXTS_INPUT = \"keyword one, keyword two, keyword three, keyword four, keyword five\" #@param {type:\"string\"}\n",
                "\n",
                "POSSIBLE_ANCHORS = [x.strip() for x in ANCHOR_TEXTS_INPUT.split(',') if x.strip()]\n",
                "\n",
                "print(f\"Targeting: {len(POSSIBLE_ANCHORS)} anchor texts\")\n",
                "print(f\"Target Page: {TARGET_URL_TO_BOOST}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0c695440",
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title ‚öôÔ∏è The Engine (Run this cell)\n",
                "import subprocess\n",
                "import sys\n",
                "import logging\n",
                "import time\n",
                "import re\n",
                "import warnings\n",
                "\n",
                "# --- 1. Silent Dependency Install ---\n",
                "def install_deps():\n",
                "    packages = [\"requests\", \"beautifulsoup4\", \"pandas\", \"scikit-learn\", \"openpyxl\", \"openai\", \"numpy\"]\n",
                "    try:\n",
                "        import requests, bs4, pandas, openai, numpy, sklearn, openpyxl\n",
                "    except ImportError:\n",
                "        print(\"Installing dependencies...\", end=\" \")\n",
                "        for p in packages:\n",
                "            try:\n",
                "                subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", p], stdout=subprocess.DEVNULL)\n",
                "            except: pass\n",
                "        print(\"Done.\")\n",
                "\n",
                "install_deps()\n",
                "\n",
                "# --- 2. Imports & Setup ---\n",
                "import requests\n",
                "import pandas as pd\n",
                "from urllib.parse import urljoin, urlparse\n",
                "from bs4 import BeautifulSoup\n",
                "from openai import OpenAI\n",
                "import numpy as np\n",
                "from sklearn.metrics.pairwise import cosine_similarity\n",
                "from google.colab import files\n",
                "import xml.etree.ElementTree as ET\n",
                "\n",
                "logging.getLogger().setLevel(logging.CRITICAL)\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "OPENAI_API_KEY = \"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\"\n",
                "\n",
                "# --- 3. Logic Classes ---\n",
                "class Crawler:\n",
                "    def __init__(self, delay: float = 0.05, max_pages: int = 300):\n",
                "        self.delay = delay\n",
                "        self.visited = set()\n",
                "        self.results = []\n",
                "        self.max_pages = max_pages\n",
                "\n",
                "    def _is_valid_url(self, url: str, base_domain: str) -> bool:\n",
                "        parsed = urlparse(url)\n",
                "        # 1. Check Domain & Scheme\n",
                "        if parsed.netloc != base_domain or parsed.scheme not in ['http', 'https']:\n",
                "            return False\n",
                "        \n",
                "        # 2. Check Exclusions (? and /page/)\n",
                "        if '?' in url or '/page/' in url:\n",
                "            return False\n",
                "            \n",
                "        # 3. Check Extensions\n",
                "        if any(url.lower().endswith(e) for e in ['.jpg', '.png', '.pdf', '.css', '.js', '.gif', '.svg', '.xml', '.zip']):\n",
                "            return False\n",
                "            \n",
                "        return True\n",
                "\n",
                "    def _fetch_sitemap_urls(self, base_url: str) -> set:\n",
                "        sitemaps = set()\n",
                "        candidates = [\n",
                "            urljoin(base_url, '/sitemap.xml'),\n",
                "            urljoin(base_url, '/sitemap_index.xml'),\n",
                "            urljoin(base_url, '/wp-sitemap.xml')\n",
                "        ]\n",
                "        for sm_url in candidates:\n",
                "            try:\n",
                "                resp = requests.get(sm_url, timeout=5)\n",
                "                if resp.status_code == 200:\n",
                "                    try:\n",
                "                        root = ET.fromstring(resp.content)\n",
                "                        for child in root.iter():\n",
                "                             if 'loc' in child.tag and child.text:\n",
                "                                sitemaps.add(child.text.strip())\n",
                "                    except: pass\n",
                "            except: pass\n",
                "        return sitemaps\n",
                "\n",
                "    def crawl(self, start_url: str) -> pd.DataFrame:\n",
                "        self.visited = set()\n",
                "        self.results = []\n",
                "        base_domain = urlparse(start_url).netloc\n",
                "        queue = [start_url]\n",
                "        self.visited.add(start_url)\n",
                "        \n",
                "        print(\"Looking for sitemaps...\", end=\" \")\n",
                "        sitemaps = self._fetch_sitemap_urls(start_url)\n",
                "        print(f\"Found {len(sitemaps)} URLs.\")\n",
                "        \n",
                "        for u in sitemaps:\n",
                "            if self._is_valid_url(u, base_domain) and u not in self.visited:\n",
                "                self.visited.add(u)\n",
                "                queue.append(u)\n",
                "        \n",
                "        print(f\"Crawling up to {self.max_pages} pages...\")\n",
                "        count = 0\n",
                "        last_print = 0\n",
                "        \n",
                "        while queue and len(self.results) < self.max_pages:\n",
                "            url = queue.pop(0)\n",
                "            count += 1\n",
                "            if count - last_print >= 20:\n",
                "                print(f\"Processed {count} URLs...\", end=\"\\r\")\n",
                "                last_print = count\n",
                "\n",
                "            try:\n",
                "                time.sleep(self.delay)\n",
                "                resp = requests.get(url, timeout=5, headers={'User-Agent': 'Mozilla/5.0'})\n",
                "                if resp.status_code == 200 and 'text/html' in resp.headers.get('Content-Type', ''):\n",
                "                    # Check noindex\n",
                "                    if 'noindex' in resp.headers.get('X-Robots-Tag', '').lower(): continue\n",
                "                    \n",
                "                    soup = BeautifulSoup(resp.text, 'html.parser')\n",
                "                    meta = soup.find('meta', attrs={'name': 'robots'})\n",
                "                    if meta and 'noindex' in meta.get('content', '').lower(): continue\n",
                "\n",
                "                    self.results.append({'url': url, 'html': resp.text})\n",
                "                    \n",
                "                    for link in soup.find_all('a', href=True):\n",
                "                        next_url = urljoin(url, link['href']).split('#')[0]\n",
                "                        if self._is_valid_url(next_url, base_domain) and next_url not in self.visited:\n",
                "                            self.visited.add(next_url)\n",
                "                            queue.append(next_url)\n",
                "            except: pass\n",
                "            \n",
                "        print(f\"\\nCrawl complete. Scanned {len(self.results)} pages.\")\n",
                "        return pd.DataFrame(self.results)\n",
                "\n",
                "def clean_soup(soup):\n",
                "    \"\"\"Removes boilerplate elements from soup in-place.\"\"\"\n",
                "    # STRICT REMOVAL of boilerplate\n",
                "    # Included 'header' and 'footer' - THIS IS KEY\n",
                "    for x in soup(['script', 'style', 'nav', 'footer', 'header', 'aside', 'form', 'noscript', 'meta', 'link']):\n",
                "        x.decompose()\n",
                "    return soup\n",
                "\n",
                "def get_embedding(text, client):\n",
                "    return client.embeddings.create(input=[text.replace(\"\\n\", \" \")], model=\"text-embedding-3-small\").data[0].embedding\n",
                "\n",
                "# --- 4. Main Execution ---\n",
                "\n",
                "if 'ROOT_URL' not in globals() or not ROOT_URL:\n",
                "    print(\"‚ùå Error: Please set the URL in the Quick Setup cell above!\")\n",
                "else:\n",
                "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
                "    crawler = Crawler()\n",
                "    df = crawler.crawl(ROOT_URL)\n",
                "    \n",
                "    if not df.empty:\n",
                "        print(\"Processing content... (this may take a minute)\")\n",
                "        \n",
                "        # Get Target Info\n",
                "        target_text = \"\"\n",
                "        try:\n",
                "            t_row = df[df['url'] == TARGET_URL_TO_BOOST]\n",
                "            if not t_row.empty:\n",
                "                t_soup = BeautifulSoup(t_row.iloc[0]['html'], 'html.parser')\n",
                "                clean_soup(t_soup)\n",
                "                target_text = t_soup.get_text(separator=' ', strip=True)\n",
                "            else:\n",
                "                resp = requests.get(TARGET_URL_TO_BOOST, headers={'User-Agent': 'Mozilla/5.0'})\n",
                "                t_soup = BeautifulSoup(resp.text, 'html.parser')\n",
                "                clean_soup(t_soup)\n",
                "                target_text = t_soup.get_text(separator=' ', strip=True)\n",
                "        except Exception as e:\n",
                "            print(f\"‚ö†Ô∏è Error fetching target page: {e}\")\n",
                "            \n",
                "        if target_text:\n",
                "            target_vec = get_embedding(target_text[:15000], client)\n",
                "            \n",
                "            # Results Containers\n",
                "            opportunities = []\n",
                "            reviews = []\n",
                "            \n",
                "            target_path = urlparse(TARGET_URL_TO_BOOST).path\n",
                "            normalized_anchors = [p.lower() for p in POSSIBLE_ANCHORS]\n",
                "\n",
                "            print(\"Analyzing pages...\")\n",
                "            for i, row in df.iterrows():\n",
                "                current_url = row['url']\n",
                "                if current_url == TARGET_URL_TO_BOOST: continue\n",
                "                \n",
                "                html = row['html']\n",
                "                soup = BeautifulSoup(html, 'html.parser')\n",
                "                \n",
                "                # --- CRITICAL FIX: Clean the soup BEFORE checking for links ---\n",
                "                # This prevents links in the nav menu or footer from being counted.\n",
                "                clean_soup(soup)\n",
                "                # ----------------------------------------------------------------\n",
                "                \n",
                "                # --- Check for Existing Links (in main content only) ---\n",
                "                has_link = False\n",
                "                link_anchor = \"\"\n",
                "                is_exact_match_link = False\n",
                "                \n",
                "                # Find links to target\n",
                "                for a in soup.find_all('a', href=True):\n",
                "                    href = a['href']\n",
                "                    abs_href = urljoin(current_url, href).split('#')[0]\n",
                "                    \n",
                "                    if abs_href == TARGET_URL_TO_BOOST or href == target_path:\n",
                "                        has_link = True\n",
                "                        anchor = a.get_text(strip=True).lower()\n",
                "                        if anchor in normalized_anchors:\n",
                "                            is_exact_match_link = True\n",
                "                            break # Keyword link found, ignore page\n",
                "                        else:\n",
                "                            link_anchor = anchor\n",
                "                \n",
                "                # --- Decision Logic ---\n",
                "                if is_exact_match_link:\n",
                "                    # Case: Link exists AND matches keyword.\n",
                "                    # Action: Ignore page.\n",
                "                    continue\n",
                "                \n",
                "                elif has_link:\n",
                "                    # Case: Link exists BUT didn't match keywords.\n",
                "                    # Action: Review.\n",
                "                    reviews.append({\n",
                "                        'Page URL': current_url,\n",
                "                        'Existing Link Anchor': link_anchor,\n",
                "                        'Action': 'Review: Change anchor?'\n",
                "                    })\n",
                "                    \n",
                "                else:\n",
                "                    # Case: No link exists in MAIN content.\n",
                "                    # Action: Standard Analysis\n",
                "                    text = soup.get_text(separator=' ', strip=True)[:15000]\n",
                "                    if not text: continue\n",
                "\n",
                "                    # A. Keyword Match\n",
                "                    matches = [k for k in POSSIBLE_ANCHORS if k.lower() in text.lower()]\n",
                "                    \n",
                "                    # B. Semantic Match\n",
                "                    score = 0\n",
                "                    try:\n",
                "                        vec = get_embedding(text, client)\n",
                "                        score = cosine_similarity([target_vec], [vec])[0][0]\n",
                "                    except: pass\n",
                "                    \n",
                "                    if matches or score > 0.73: # Slightly lowered threshold\n",
                "                        opportunities.append({\n",
                "                            'Page URL': current_url,\n",
                "                            'Target URL': TARGET_URL_TO_BOOST,\n",
                "                            'Keywords Found': \", \".join(matches),\n",
                "                            'Semantic Score': round(score, 4),\n",
                "                            'Type': 'Keyword' if matches else 'Semantic'\n",
                "                        })\n",
                "\n",
                "            # --- Export ---\n",
                "            if opportunities or reviews:\n",
                "                filename = \"link_opportunities_v3.xlsx\"\n",
                "                with pd.ExcelWriter(filename) as writer:\n",
                "                    if opportunities:\n",
                "                        pd.DataFrame(opportunities).sort_values('Semantic Score', ascending=False).to_excel(writer, sheet_name='New Opportunities', index=False)\n",
                "                    if reviews:\n",
                "                        pd.DataFrame(reviews).to_excel(writer, sheet_name='Existing Link Review', index=False)\n",
                "                \n",
                "                print(f\"\\n‚úÖ Done! Found {len(opportunities)} new opportunities and {len(reviews)} pages to review.\")\n",
                "                files.download(filename)\n",
                "            else:\n",
                "                print(\"\\n‚ÑπÔ∏è No matching opportunities or reviews found.\")\n",
                "\n",
                "        else:\n",
                "            print(\"‚ùå Could not analyze target page content.\")\n",
                "    else:\n",
                "        print(\"‚ùå No pages found to analyze.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
